{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for image classification analysis\n",
    "In this notebook a CNN model will be developed to classify bark according to tree species. The CNN will be trained to classify the original images. After the training the CNN will be evaluated with some post-hoc model analysis methods like LIME and SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing the data structure (only done once, after downloading the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the dataset (https://www.kaggle.com/datasets/saurabhshahane/barkvn50) should be downloaded to directory: \"./data/BarkVN-50/\" and unzipped. You should then have the following structure:\n",
    "- data\n",
    "    - BarkVN-50\n",
    "        - BarkVN-50_mendeley\n",
    "            - Acacia\n",
    "            - Adenanthera microsperma\n",
    "            - Adenieum species\n",
    "            - Anacardium occidentale\n",
    "            - ...\n",
    "\n",
    "Since this is not ideal for this CNN, a subset of the data is selected and split into training data using the code in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helpers.split\n",
    "# helpers.split.train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this cell only needs to be executed once (this is why it is commented out by default).\n",
    "\n",
    "After execution the new data structure looks like this:\n",
    "- data\n",
    "    - BarkVN-50\n",
    "        - BarkVN-50_mendeley\n",
    "            - Acacia\n",
    "            - Adenanthera microsperma\n",
    "            - Adenieum species\n",
    "            - Anacardium occidentale\n",
    "            - ...\n",
    "        - Test\n",
    "            - Adenanthera microsperma\n",
    "            - Cananga odorata\n",
    "            - Cedrus\n",
    "            - Cocos nucifera\n",
    "            - Dalvergia oliveri\n",
    "        - Train\n",
    "            - Adenanthera microsperma\n",
    "            - Cananga odorata\n",
    "            - Cedrus\n",
    "            - Cocos nucifera\n",
    "            - Dalvergia oliveri\n",
    "\n",
    "Note: the directory \"./data/BarkVN-50/BarkVN-50_mendeley\" may be deleted after this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset and creating DataLoaders\n",
    "Since the used dataset is a custom one, we need to first create a custom Dataset for loading, transforming and delivering datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import BarkVN50Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import device\n",
    "from torch.cuda import is_available\n",
    "\n",
    "# recognizing device\n",
    "DEVICE = device(\"cuda\" if is_available() else \"cpu\")\n",
    "\n",
    "# load train and test dataset\n",
    "train_dataset = BarkVN50Dataset(train=True, device=DEVICE)\n",
    "test_dataset = BarkVN50Dataset(train=False, device=DEVICE)\n",
    "\n",
    "# create DataLoaders that automatically create minibatches and shuffle the data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=39, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=39, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Now that the data is ready to be used, we can load the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNeuralNetwork(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (categorizer): Sequential(\n",
       "    (0): Linear(in_features=60600, out_features=4800, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4800, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.cnn import ConvolutionalNeuralNetwork\n",
    "\n",
    "model = ConvolutionalNeuralNetwork()\n",
    "model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the hyperaparameters, optimizer and the criterion (loss function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 3e-5\n",
    "\n",
    "# optimizer and loss function\n",
    "model.train()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN model\n",
    "And finally train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/11], Loss: 7.6943\n",
      "Epoch [1/10], Step [2/11], Loss: 104.4559\n",
      "Epoch [1/10], Step [3/11], Loss: 191.0229\n",
      "Epoch [1/10], Step [4/11], Loss: 305.4359\n",
      "Epoch [1/10], Step [5/11], Loss: 252.1283\n",
      "Epoch [1/10], Step [6/11], Loss: 168.2683\n",
      "Epoch [1/10], Step [7/11], Loss: 138.5073\n",
      "Epoch [1/10], Step [8/11], Loss: 163.5892\n",
      "Epoch [1/10], Step [9/11], Loss: 97.1250\n",
      "Epoch [1/10], Step [10/11], Loss: 97.0325\n",
      "Epoch [1/10], Step [11/11], Loss: 86.0170\n",
      "Epoch [2/10], Step [1/11], Loss: 80.2032\n",
      "Epoch [2/10], Step [2/11], Loss: 43.2833\n",
      "Epoch [2/10], Step [3/11], Loss: 28.1741\n",
      "Epoch [2/10], Step [4/11], Loss: 33.0421\n",
      "Epoch [2/10], Step [5/11], Loss: 29.8745\n",
      "Epoch [2/10], Step [6/11], Loss: 14.1201\n",
      "Epoch [2/10], Step [7/11], Loss: 15.5952\n",
      "Epoch [2/10], Step [8/11], Loss: 27.9101\n",
      "Epoch [2/10], Step [9/11], Loss: 26.9730\n",
      "Epoch [2/10], Step [10/11], Loss: 31.8553\n",
      "Epoch [2/10], Step [11/11], Loss: 30.2992\n",
      "Epoch [3/10], Step [1/11], Loss: 35.8994\n",
      "Epoch [3/10], Step [2/11], Loss: 10.8097\n",
      "Epoch [3/10], Step [3/11], Loss: 15.2293\n",
      "Epoch [3/10], Step [4/11], Loss: 11.9927\n",
      "Epoch [3/10], Step [5/11], Loss: 6.5648\n",
      "Epoch [3/10], Step [6/11], Loss: 13.3173\n",
      "Epoch [3/10], Step [7/11], Loss: 13.3802\n",
      "Epoch [3/10], Step [8/11], Loss: 16.1766\n",
      "Epoch [3/10], Step [9/11], Loss: 19.7311\n",
      "Epoch [3/10], Step [10/11], Loss: 15.3577\n",
      "Epoch [3/10], Step [11/11], Loss: 14.1385\n",
      "Epoch [4/10], Step [1/11], Loss: 6.3651\n",
      "Epoch [4/10], Step [2/11], Loss: 8.2258\n",
      "Epoch [4/10], Step [3/11], Loss: 6.6650\n",
      "Epoch [4/10], Step [4/11], Loss: 9.9480\n",
      "Epoch [4/10], Step [5/11], Loss: 7.3156\n",
      "Epoch [4/10], Step [6/11], Loss: 8.1786\n",
      "Epoch [4/10], Step [7/11], Loss: 3.4872\n",
      "Epoch [4/10], Step [8/11], Loss: 3.7926\n",
      "Epoch [4/10], Step [9/11], Loss: 4.4715\n",
      "Epoch [4/10], Step [10/11], Loss: 7.2721\n",
      "Epoch [4/10], Step [11/11], Loss: 5.6963\n",
      "Epoch [5/10], Step [1/11], Loss: 2.7190\n",
      "Epoch [5/10], Step [2/11], Loss: 4.1156\n",
      "Epoch [5/10], Step [3/11], Loss: 4.2280\n",
      "Epoch [5/10], Step [4/11], Loss: 2.6408\n",
      "Epoch [5/10], Step [5/11], Loss: 3.8313\n",
      "Epoch [5/10], Step [6/11], Loss: 5.2528\n",
      "Epoch [5/10], Step [7/11], Loss: 2.1331\n",
      "Epoch [5/10], Step [8/11], Loss: 7.3746\n",
      "Epoch [5/10], Step [9/11], Loss: 3.6329\n",
      "Epoch [5/10], Step [10/11], Loss: 3.6833\n",
      "Epoch [5/10], Step [11/11], Loss: 5.9892\n",
      "Epoch [6/10], Step [1/11], Loss: 7.3393\n",
      "Epoch [6/10], Step [2/11], Loss: 2.5218\n",
      "Epoch [6/10], Step [3/11], Loss: 3.6287\n",
      "Epoch [6/10], Step [4/11], Loss: 5.6630\n",
      "Epoch [6/10], Step [5/11], Loss: 4.7953\n",
      "Epoch [6/10], Step [6/11], Loss: 3.1865\n",
      "Epoch [6/10], Step [7/11], Loss: 2.7105\n",
      "Epoch [6/10], Step [8/11], Loss: 3.8842\n",
      "Epoch [6/10], Step [9/11], Loss: 3.9361\n",
      "Epoch [6/10], Step [10/11], Loss: 4.0098\n",
      "Epoch [6/10], Step [11/11], Loss: 2.5445\n",
      "Epoch [7/10], Step [1/11], Loss: 2.9318\n",
      "Epoch [7/10], Step [2/11], Loss: 0.7339\n",
      "Epoch [7/10], Step [3/11], Loss: 2.3770\n",
      "Epoch [7/10], Step [4/11], Loss: 1.5044\n",
      "Epoch [7/10], Step [5/11], Loss: 0.8869\n",
      "Epoch [7/10], Step [6/11], Loss: 1.5144\n",
      "Epoch [7/10], Step [7/11], Loss: 1.9934\n",
      "Epoch [7/10], Step [8/11], Loss: 0.8914\n",
      "Epoch [7/10], Step [9/11], Loss: 1.9392\n",
      "Epoch [7/10], Step [10/11], Loss: 1.4036\n",
      "Epoch [7/10], Step [11/11], Loss: 0.5585\n",
      "Epoch [8/10], Step [1/11], Loss: 1.2248\n",
      "Epoch [8/10], Step [2/11], Loss: 1.4912\n",
      "Epoch [8/10], Step [3/11], Loss: 1.2686\n",
      "Epoch [8/10], Step [4/11], Loss: 0.6607\n",
      "Epoch [8/10], Step [5/11], Loss: 1.2837\n",
      "Epoch [8/10], Step [6/11], Loss: 0.6431\n",
      "Epoch [8/10], Step [7/11], Loss: 0.5969\n",
      "Epoch [8/10], Step [8/11], Loss: 1.3000\n",
      "Epoch [8/10], Step [9/11], Loss: 0.4103\n",
      "Epoch [8/10], Step [10/11], Loss: 0.3838\n",
      "Epoch [8/10], Step [11/11], Loss: 0.7906\n",
      "Epoch [9/10], Step [1/11], Loss: 0.3902\n",
      "Epoch [9/10], Step [2/11], Loss: 0.4890\n",
      "Epoch [9/10], Step [3/11], Loss: 0.3552\n",
      "Epoch [9/10], Step [4/11], Loss: 0.3254\n",
      "Epoch [9/10], Step [5/11], Loss: 0.2939\n",
      "Epoch [9/10], Step [6/11], Loss: 0.4194\n",
      "Epoch [9/10], Step [7/11], Loss: 0.2395\n",
      "Epoch [9/10], Step [8/11], Loss: 0.5068\n",
      "Epoch [9/10], Step [9/11], Loss: 0.4101\n",
      "Epoch [9/10], Step [10/11], Loss: 0.3191\n",
      "Epoch [9/10], Step [11/11], Loss: 0.3968\n",
      "Epoch [10/10], Step [1/11], Loss: 0.3227\n",
      "Epoch [10/10], Step [2/11], Loss: 0.1891\n",
      "Epoch [10/10], Step [3/11], Loss: 0.2111\n",
      "Epoch [10/10], Step [4/11], Loss: 0.4051\n",
      "Epoch [10/10], Step [5/11], Loss: 0.1961\n",
      "Epoch [10/10], Step [6/11], Loss: 0.1318\n",
      "Epoch [10/10], Step [7/11], Loss: 0.1905\n",
      "Epoch [10/10], Step [8/11], Loss: 0.2620\n",
      "Epoch [10/10], Step [9/11], Loss: 0.2759\n",
      "Epoch [10/10], Step [10/11], Loss: 0.3824\n",
      "Epoch [10/10], Step [11/11], Loss: 0.1444\n"
     ]
    }
   ],
   "source": [
    "from helpers.train import train_cnn\n",
    "\n",
    "train_cnn(\n",
    "    num_epochs=num_epochs,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model should be serializable and deserializable, it can be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from datetime import datetime\n",
    "\n",
    "time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "save(model.state_dict(), f\"models/{time}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
