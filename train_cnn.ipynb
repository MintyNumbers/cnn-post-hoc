{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN model for tree classification based on images of their bark\n",
    "In this notebook a CNN model will be developed to classify bark according to tree species. The CNN will be trained to classify the original images. After the training the CNN will be evaluated with some post-hoc model analysis methods like LIME and SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing the data structure (only done once, after downloading the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the dataset (https://www.kaggle.com/datasets/saurabhshahane/barkvn50) should be downloaded to directory: \"./data/BarkVN-50/\" and unzipped. You should then have the following structure:\n",
    "- data\n",
    "    - BarkVN-50\n",
    "        - BarkVN-50_mendeley\n",
    "            - Acacia\n",
    "            - Adenanthera microsperma\n",
    "            - Adenieum species\n",
    "            - Anacardium occidentale\n",
    "            - ...\n",
    "\n",
    "Since this is not ideal for this CNN, a subset of the data is selected and split into training data using the code in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helpers.split\n",
    "# helpers.split.train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this cell only needs to be executed once (this is why it is commented out by default).\n",
    "\n",
    "After execution the new data structure looks like this:\n",
    "- data\n",
    "    - BarkVN-50\n",
    "        - BarkVN-50_mendeley\n",
    "            - Acacia\n",
    "            - Adenanthera microsperma\n",
    "            - Adenieum species\n",
    "            - Anacardium occidentale\n",
    "            - ...\n",
    "        - Test\n",
    "            - Adenanthera microsperma\n",
    "            - Cananga odorata\n",
    "            - Cedrus\n",
    "            - Cocos nucifera\n",
    "            - Dalvergia oliveri\n",
    "        - Train\n",
    "            - Adenanthera microsperma\n",
    "            - Cananga odorata\n",
    "            - Cedrus\n",
    "            - Cocos nucifera\n",
    "            - Dalvergia oliveri\n",
    "\n",
    "Note: the directory \"./data/BarkVN-50/BarkVN-50_mendeley\" may be deleted after this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset and creating DataLoaders\n",
    "Since the used dataset is a custom one, we need to first create a custom Dataset for loading, transforming and delivering datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import BarkVN50Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import device, manual_seed\n",
    "from torch.cuda import is_available\n",
    "\n",
    "# setting random seed\n",
    "manual_seed(0)\n",
    "\n",
    "# recognizing device\n",
    "DEVICE = device(\"cuda\" if is_available() else \"cpu\")\n",
    "\n",
    "# load train dataset and create DataLoaders that automatically create minibatches and shuffle the data\n",
    "train_dataset = BarkVN50Dataset(train=True, device=DEVICE)\n",
    "test_dataset = BarkVN50Dataset(train=False, device=DEVICE)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN model\n",
    "To train the model we can use either the Train/Test split or the K-Fold Cross Validation split. In my case I first split the dataset into a Train and Test Subset and will mostly be training the model with KF CV. This has the advantage that I will always have a dataset I can use to assess all of the models separately and also see how much the model's performance depends on a lucky/unlucky dataset split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN model with Train/Test Split...\n",
    "This chapter will show how to train a single model (either a wholly new one or a pre-trained one) using the Train/Test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.cnn import ConvolutionalNeuralNetwork\n",
    "from torch.optim import Adam\n",
    "\n",
    "# model and optimizer\n",
    "model = ConvolutionalNeuralNetwork().to(device=DEVICE)\n",
    "optimizer = Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an already existing model should be trained further, it can be loaded from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import load\n",
    "\n",
    "# checkpoint = load(\"models/checkpoint-2024-11-04-18-14-59.tar\", weights_only=True)\n",
    "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "# epoch = checkpoint[\"epoch\"]\n",
    "# loss = checkpoint[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.train import train_cnn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model.train()\n",
    "num_epochs = 15\n",
    "loss = train_cnn(\n",
    "    num_epochs=num_epochs,\n",
    "    model=model,\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model should be trained again later on, it can be saved using the .tar (PyTorch convention for model checkpoints) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from datetime import datetime\n",
    "\n",
    "time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# If the model should be trainable\n",
    "save(\n",
    "    {\n",
    "        \"epoch\": num_epochs,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    },\n",
    "    f\"models/checkpoint-{num_epochs}ep-{time}.tar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if it shouldn't be trainable, but nonetheless be evaluated, only the model's state_dictionary can be saved with the .pt format (PyTorch convention for finished models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from datetime import datetime\n",
    "\n",
    "# If the model will only be used for inference (requires 2-3 times less storage than the other save option)\n",
    "time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "save(model.state_dict(), f\"models/eval-model-{num_epochs}ep-{time}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...or training it using K-Fold Cross Validation\n",
    "Another option would be to train the model using K-Fold Cross Validation. This has the advantage of evaluating the model more thoroughly thus reducing the \"lucky/unlucky split\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.kfold import train_cnn_kfold\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "train_cnn_kfold(\n",
    "    epoch_per_kfold=10,\n",
    "    num_kfold=10,\n",
    "    train_dataset=train_dataset,\n",
    "    test_loader=test_dataloader,\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model's accuracy on test data\n",
    "Now that a CNN model has been trained it is time to evaluate it on original data, test data as well as data altered with noise and/or overlapping pixels. Using LIME and SHAP the CNN's classification and created heatmap changes will be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved model for evaluation, the following commands can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load\n",
    "from helpers.cnn import ConvolutionalNeuralNetwork\n",
    "\n",
    "model = ConvolutionalNeuralNetwork().to(device=DEVICE)\n",
    "# model.load_state_dict(load(\"models/ignoring-0/eval-model-20ep-2024-11-15-20-38-07.pt\", weights_only=True))\n",
    "model.load_state_dict(load(\"models/kfold-2024-11-19-15-15-12-1.tar\", weights_only=True)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluation with more complex algorithms, it is helpful to visualize the model's performance with a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.evaluate import evaluate_cnn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model.eval()\n",
    "evaluate_cnn(\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    test_dataloader=test_dataloader,\n",
    "    train_dataloader=train_dataloader,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation with SHAP and LIME\n",
    "Okay, now that we see how the trained CNN performs it is time to see *why* the CNN performs how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.shap import shap_evaluate_cnn\n",
    "\n",
    "shap_evaluate_cnn(model=model, test_dataset=test_dataset, train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.lime import lime_evaluate_cnn\n",
    "\n",
    "lime_evaluate_cnn(model=model, test_dataset=test_dataset, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation with activation hooks\n",
    "To evaluate the model's acivations we need to first setup forward hooks on every layer of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.activations import setup_hooks\n",
    "\n",
    "activations = setup_hooks(model, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the forward hooks in place, we can choose a random image and put it through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from numpy.random import randint\n",
    "\n",
    "# Select random OR specific image\n",
    "# index = randint(0, len(test_dataset.images))\n",
    "index = 0\n",
    "image = test_dataset.images[index : index + 1]\n",
    "label = test_dataset.labels[index]\n",
    "\n",
    "# Display the input image\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"Input Image - Label: {label}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Run the model on the sample image\n",
    "output = model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implemented hooks have recorded the data from this forward pass. We can now visualize it.\n",
    "\n",
    "The first cell plots the FC layer at the end of the model. It shows the model's prediction of how likely it is that the model belongs to each of the classes.\n",
    "\n",
    "The second and third cells will plot the activations of the first and second convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.activations import plot_fc_activations\n",
    "\n",
    "# Get activations from FC layer\n",
    "plot_fc_activations(activations, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.activations import plot_conv_activations\n",
    "\n",
    "# Get activations from the first convolutional layer\n",
    "plot_conv_activations(activations, \"cnn0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations from the second convolutional layer\n",
    "plot_conv_activations(activations, \"cnn4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze the layer activations of the model, we can generate a random image and run the model on it. Using the Adam optimizer the model will then be adjusted after each epoch to alter it in such a way that the selected layer's activation is maximized. This function will plot images that maximize the activation for different filters of the selected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.activations\n",
    "from importlib import reload\n",
    "\n",
    "reload(helpers.activations)\n",
    "\n",
    "# Maximize activations for the first ReLU layer\n",
    "helpers.activations.filter_activation_maximization(\n",
    "    cnn_layer_num=1,\n",
    "    model=model,\n",
    "    input_size=(1, 1, 404, 303),\n",
    "    lr=0.1,\n",
    "    iterations=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize activations for the second ReLU layer\n",
    "helpers.activations.filter_activation_maximization(\n",
    "    cnn_layer_num=5,\n",
    "    model=model,\n",
    "    input_size=(1, 1, 404, 303),\n",
    "    lr=0.1,\n",
    "    iterations=100,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
