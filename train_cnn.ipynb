{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN model for tree classification based on images of their bark\n",
    "In this notebook a CNN model will be developed to classify bark according to tree species. The CNN will be trained to classify the original images. After the training the CNN will be evaluated with some post-hoc model analysis methods like LIME and SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing the data structure (only done once, after downloading the dataset)\n",
    "First, the dataset (https://www.kaggle.com/datasets/saurabhshahane/barkvn50) should be downloaded to directory: \"./data/BarkVN-50/\" and unzipped. You should then have the following structure:\n",
    "- data\n",
    "    - BarkVN-50\n",
    "        - BarkVN-50_mendeley\n",
    "            - Acacia\n",
    "            - Adenanthera microsperma\n",
    "            - Adenieum species\n",
    "            - Anacardium occidentale\n",
    "            - ...\n",
    "\n",
    "Since this is not ideal for this CNN, a subset of the data (with at least 109 samples and image size 404*303, resulting in 10 species) is selected and split into training data using the code in the next cell. After the execution you should have another two directories under \"./data/BarkVN-50/\" - a Test and a Train directory. Each Train species subdirectory will contain directories with 99 samples of the species' bark. The Test subdirectory will contain 10.\n",
    "\n",
    "The tree species that you can expect will be copied to the Train and Test subdirectories are: Cocos nucifera (0), Dipterocarpus alatus (1), Eucalyptus (2), Ficus microcarpa (3), Hevea brasiliensis (4), Musa (5), Psidium guajava (6), Syzygium nervosum (7), Terminalia catappa (8), Veitchia merrilli (9).\n",
    "\n",
    "Note 1: this cell only needs to be executed once (this is why it is commented out by default).\n",
    "\n",
    "Note 2: the directory \"./data/BarkVN-50/BarkVN-50_mendeley\" may be deleted after this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helpers.split\n",
    "\n",
    "# helpers.split.train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset and creating DataLoaders\n",
    "Since the used dataset is a custom one, we need to first create a custom Dataset for loading, transforming and delivering datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import BarkVN50Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import device, manual_seed\n",
    "from torch.cuda import is_available\n",
    "from torchvision.transforms.v2 import Compose, RandomVerticalFlip, RandomHorizontalFlip, RandomRotation\n",
    "\n",
    "# setting random seed\n",
    "manual_seed(0)\n",
    "\n",
    "# recognizing device\n",
    "DEVICE = device(\"cuda\" if is_available() else \"cpu\")\n",
    "\n",
    "# load train dataset and create DataLoaders that automatically create minibatches and shuffle the data\n",
    "transforms = Compose([RandomVerticalFlip(p=0.5), RandomHorizontalFlip(p=0.5), RandomRotation(30)])\n",
    "train_dataset = BarkVN50Dataset(train=True, device=DEVICE, transforms=transforms)\n",
    "test_dataset = BarkVN50Dataset(train=False, device=DEVICE, transforms=None)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN model\n",
    "To train the model we can use either the Train/Test split or the K-Fold Cross Validation split. In my case I first split the dataset into a Train and Test Subset and will mostly be training the model with KF CV. This has the advantage that I will always have a dataset I can use to assess all of the models separately and also see how much the model's performance depends on a lucky/unlucky dataset split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a CNN model with Train/Test Split\n",
    "This chapter will show how to train a single model (either a wholly new one or a pre-trained one) using the Train/Test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.cnn import ConvolutionalNeuralNetwork\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# model and optimizer\n",
    "model = ConvolutionalNeuralNetwork().to(device=DEVICE)\n",
    "optimizer = Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "# optimizer = SGD(model.parameters(), lr=3e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an already existing model should be trained for more epochs, it can be loaded from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import load\n",
    "\n",
    "# checkpoint = load(\"models/checkpoint-2024-11-04-18-14-59.tar\", weights_only=True)\n",
    "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "# epoch = checkpoint[\"epoch\"]\n",
    "# loss = checkpoint[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once the model is initialized (and weights have also been loaded from disk), the model training can begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.train import train_cnn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model.train()\n",
    "num_epochs = 50\n",
    "loss = train_cnn(\n",
    "    num_epochs=num_epochs,\n",
    "    model=model,\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "If the model should be trained again later on, it can be saved using the .tar (PyTorch convention for model checkpoints) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from datetime import datetime\n",
    "\n",
    "time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# If the model should be trainable\n",
    "save(\n",
    "    {\n",
    "        \"epoch\": num_epochs,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    },\n",
    "    f\"models/checkpoint-{num_epochs}ep-{time}.tar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if it shouldn't be trainable, but nonetheless be evaluated, only the model's state_dictionary can be saved with the .pt format (PyTorch convention for finished models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from datetime import datetime\n",
    "\n",
    "# If the model will only be used for inference (requires 2-3 times less storage than the other save option)\n",
    "time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "save(model.state_dict(), f\"models/eval-model-{num_epochs}ep-{time}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training k models using K-Fold Cross Validation\n",
    "Another option would be to train the model using K-Fold Cross Validation. This has the advantage of evaluating the model more thoroughly by reducing the \"lucky/unlucky split\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.kfold import train_cnn_kfold\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "train_cnn_kfold(\n",
    "    epoch_per_kfold=5,\n",
    "    num_kfold=10,\n",
    "    train_dataset=train_dataset,\n",
    "    test_loader=test_dataloader,\n",
    "    sgd=True,\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model's accuracy on test data\n",
    "Now that a CNN model has been trained it is time to evaluate it on original data, test data as well as data altered with noise and/or overlapping pixels. Using LIME and SHAP the CNN's classification and created heatmap changes will be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved model for evaluation, the following commands can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load\n",
    "from helpers.cnn import ConvolutionalNeuralNetwork\n",
    "\n",
    "model = ConvolutionalNeuralNetwork().to(device=DEVICE)\n",
    "# model.load_state_dict(load(\"models/ignoring-0/eval-model-20ep-2024-11-15-20-38-07.pt\", weights_only=True))\n",
    "model.load_state_dict(\n",
    "    load(\"models/good-model/checkpoint-50ep-2024-11-20-17-00-09.tar\", weights_only=True)[\"model_state_dict\"]\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluation with more complex algorithms, it is helpful to visualize the model's performance with a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.evaluate import evaluate_cnn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "evaluate_cnn(\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    test_dataloader=test_dataloader,\n",
    "    train_dataloader=train_dataloader,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation with SHAP and LIME\n",
    "Okay, now that we see how the trained CNN performs it is time to see *why* the CNN performs how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.shap import shap_evaluate_cnn\n",
    "\n",
    "shap_evaluate_cnn(model=model, test_dataset=test_dataset, train_dataset=train_dataset, test_image_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.lime import lime_evaluate_cnn\n",
    "\n",
    "lime_evaluate_cnn(model=model, test_dataset=test_dataset, device=DEVICE, augmented=True, test_image_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation with activation hooks\n",
    "To further analyze the model's performance we can view its layers' activations after an image was passed through the model. The implemented hooks will record the data from a forward pass. This will be visualized by the plot_image_activations function.\n",
    "\n",
    "The first image displays the input.\n",
    "\n",
    "The second image shows the activations of the FC layer at the end of the model. It shows the model's prediction of how likely it is that the model belongs to each of the classes.\n",
    "\n",
    "The third and fourth images plot the activations of the first and second convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.activations import setup_hooks, plot_image_activations\n",
    "\n",
    "activations = setup_hooks(model, {})\n",
    "plot_image_activations(activations=activations, index=0, model=model, test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze the layer activations of the model, we can generate a random image and run the model on it. Using the Adam optimizer the model will then be adjusted after each epoch to alter it in such a way that the selected layer's activation is maximized. This function will plot images that maximize the activation for different filters of the selected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.activations import filter_activation_maximization\n",
    "\n",
    "# Maximize activations for the first ReLU layer\n",
    "filter_activation_maximization(cnn_layer_num=1, model=model, input_size=(1, 1, 404, 303), lr=0.1, iterations=100)\n",
    "\n",
    "# Maximize activations for the second ReLU layer\n",
    "filter_activation_maximization(cnn_layer_num=5, model=model, input_size=(1, 1, 404, 303), lr=0.1, iterations=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
